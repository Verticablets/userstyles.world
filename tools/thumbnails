#!/usr/bin/env sh

data="data-links"
images="data-images"
thumbs="data-thumbs"

log() { printf "%s\n" "$@"; }

scrape() {
    log "Scraping images..."

    grep -h "preview" ../content/styles/**/*md | awk '{ print $2 }' > "$data"

    log "Done!"
}

download() {
    log "Downloading images..."

    mkdir -p "$images"

    while read -r line; do
        # Remove `https://` then replace all `/` with `_` so that the downloaded
        # image can be saved in the `$images` directory.
        escaped=$( printf "%s" "$line" | sed -e 's/https\?:\/\///g; s/\//_/g' )

        # TODO: Implement a comparsion to avoid re-downloading images.

        # Download the images.
        curl -# -o "$images/$escaped" -L "$line"
    done < "$data"

    log "Done!"
}

resize() {
    log "Making thumbnails..."

    mkdir -p "$thumbs"

    for f in "$images"/*png; do
        output=${f##"$images/"}
        printf "I: %s\nO: %s\n\n" "$f" "$thumbs/$output"
        convert "$f" -resize 309x173 "$thumbs/$output"
    done

    log "Done!"
}

clean() {
    log "Cleaning files..."

    rm -r "$images" "$thumbs" "$data"

    log "Done!"
}

all() {
    scrape
    log ""
    download
    log ""
    resize
}

if [ "$#" -eq 0 ]; then
    all
    exit 0
fi

case "$1" in
    h|help) printf "See the source code.\n" ;;
    g|rg|grep|scrape) scrape "$@" ;;
    d|dl|download) download "$@" ;;
    r|resize) resize "$@" ;;
    c|clean) clean "$@" ;;
    a|all) all "$@" ;;
    *) printf "Invalid option: %s\n" "$*" ;;
esac
